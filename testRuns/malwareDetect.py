import numpy as np
import pandas as pd
from sklearn.model_selection import train_test_split

# For ML
from sklearn.linear_model import LogisticRegression
from sklearn.svm import SVC
from sklearn import tree
from sklearn import naive_bayes
from sklearn.ensemble import RandomForestClassifier
from sklearn.neural_network import MLPClassifier
from sklearn.metrics import accuracy_score
from mlens.model_selection import Evaluator
from sklearn.naive_bayes import GaussianNB
from sklearn.neighbors import KNeighborsClassifier
from scipy.stats import randint
import sklearn.model_selection
from mlens.ensemble import SuperLearner

import pickle
import sys
import os

datasetPath = "/home/zin/lab/datasets/N-BaIoT/"

benign = pd.read_csv(os.path.join(datasetPath, "8.benign.csv"))
g_c = pd.read_csv(os.path.join(datasetPath, "8.gafgyt.combo.csv"))
g_j = pd.read_csv(os.path.join(datasetPath, "8.gafgyt.junk.csv"))
g_s = pd.read_csv(os.path.join(datasetPath, "8.gafgyt.scan.csv"))
g_t = pd.read_csv(os.path.join(datasetPath, "8.gafgyt.tcp.csv"))
g_u = pd.read_csv(os.path.join(datasetPath, "8.gafgyt.udp.csv"))
ack = pd.read_csv(os.path.join(datasetPath, "8.mirai.ack.csv"))
sca = pd.read_csv(os.path.join(datasetPath, "8.mirai.scan.csv"))
syn = pd.read_csv(os.path.join(datasetPath, "8.mirai.syn.csv"))
udp = pd.read_csv(os.path.join(datasetPath, "8.mirai.udp.csv"))
pln = pd.read_csv(os.path.join(datasetPath, "8.mirai.udpplain.csv"))

benign7 = pd.read_csv(os.path.join(datasetPath, "7.benign.csv"))
g_c7 = pd.read_csv(os.path.join(datasetPath, "7.gafgyt.combo.csv"))
g_j7 = pd.read_csv(os.path.join(datasetPath, "7.gafgyt.junk.csv"))
g_s7 = pd.read_csv(os.path.join(datasetPath, "7.gafgyt.scan.csv"))
g_t7 = pd.read_csv(os.path.join(datasetPath, "7.gafgyt.tcp.csv"))
g_u7 = pd.read_csv(os.path.join(datasetPath, "7.gafgyt.udp.csv"))

benign1 = pd.read_csv(os.path.join(datasetPath, "1.benign.csv"))
g_c1 = pd.read_csv(os.path.join(datasetPath, "1.gafgyt.combo.csv"))
g_j1 = pd.read_csv(os.path.join(datasetPath, "1.gafgyt.junk.csv"))
g_s1 = pd.read_csv(os.path.join(datasetPath, "1.gafgyt.scan.csv"))
g_t1 = pd.read_csv(os.path.join(datasetPath, "1.gafgyt.tcp.csv"))
g_u1 = pd.read_csv(os.path.join(datasetPath, "1.gafgyt.udp.csv"))
ack1 = pd.read_csv(os.path.join(datasetPath, "1.mirai.ack.csv"))
sca1 = pd.read_csv(os.path.join(datasetPath, "1.mirai.scan.csv"))
syn1 = pd.read_csv(os.path.join(datasetPath, "1.mirai.syn.csv"))
udp1 = pd.read_csv(os.path.join(datasetPath, "1.mirai.udp.csv"))
pln1 = pd.read_csv(os.path.join(datasetPath, "1.mirai.udpplain.csv"))

benign2 = pd.read_csv(os.path.join(datasetPath, "2.benign.csv"))
g_c2 = pd.read_csv(os.path.join(datasetPath, "2.gafgyt.combo.csv"))
g_j2 = pd.read_csv(os.path.join(datasetPath, "2.gafgyt.junk.csv"))
g_s2 = pd.read_csv(os.path.join(datasetPath, "2.gafgyt.scan.csv"))
g_t2 = pd.read_csv(os.path.join(datasetPath, "2.gafgyt.tcp.csv"))
g_u2 = pd.read_csv(os.path.join(datasetPath, "2.gafgyt.udp.csv"))
ack2 = pd.read_csv(os.path.join(datasetPath, "2.mirai.ack.csv"))
sca2 = pd.read_csv(os.path.join(datasetPath, "2.mirai.scan.csv"))
syn2 = pd.read_csv(os.path.join(datasetPath, "2.mirai.syn.csv"))
udp2 = pd.read_csv(os.path.join(datasetPath, "2.mirai.udp.csv"))
pln2 = pd.read_csv(os.path.join(datasetPath, "2.mirai.udpplain.csv"))

frames = [g_c, g_c7, g_c1, g_c2]
frames1 = [g_j, g_j7, g_j1, g_j2]
frames2 = [g_s, g_s7, g_s1, g_s2]
frames3 = [g_t, g_t7, g_t1, g_t2]
frames4 = [g_u, g_u7, g_u1, g_u2]
frames5 = [ack, ack2, ack1]
frames6 = [sca, sca2, sca1]
frames7 = [syn, syn2, syn1]
frames8 = [udp, udp2, udp1]
frames9 = [pln, pln2, pln1]
frames0 = [benign, benign7, benign1, benign2]

benign = pd.concat(frames0)
g_c = pd.concat(frames)
g_j = pd.concat(frames1)
g_s = pd.concat(frames2)
g_t = pd.concat(frames3)
g_u = pd.concat(frames4)
ack = pd.concat(frames5)
sca = pd.concat(frames6)
syn = pd.concat(frames7)
udp = pd.concat(frames8)
pln = pd.concat(frames9)
benign = benign.sample(frac=0.25, replace=False)
g_c = g_c.sample(frac=0.25, replace=False)
g_j = g_j.sample(frac=0.5, replace=False)
g_s = g_s.sample(frac=0.5, replace=False)
g_t = g_t.sample(frac=0.15, replace=False)
g_u = g_u.sample(frac=0.15, replace=False)
ack = ack.sample(frac=0.15, replace=False)
sca = sca.sample(frac=0.15, replace=False)
syn = syn.sample(frac=0.15, replace=False)
udp = udp.sample(frac=0.15, replace=False)
pln = pln.sample(frac=0.15, replace=False)

benign["type"] = "benign"
g_c["type"] = "gafgyt_combo"
g_j["type"] = "gafgyt_junk"
g_s["type"] = "gafgyt_scan"
g_t["type"] = "gafgyt_tcp"
g_u["type"] = "gafgyt_udp"
ack["type"] = "mirai_ack"
sca["type"] = "mirai_scan"
syn["type"] = "mirai_syn"
udp["type"] = "mirai_udp"
pln["type"] = "mirai_udpplain"

data = pd.concat(
    [benign, g_c, g_j, g_s, g_t, g_u, ack, sca, syn, udp, pln],
    axis=0,
    sort=False,
    ignore_index=True,
)

data.groupby("type")["type"].count()


# shuffle rows of dataframe
sampler = np.random.permutation(len(data))
data = data.take(sampler)
data.head(20)


# dummy encode labels, store separately
labels_full = pd.get_dummies(data["type"], prefix="type")
labels_full = labels_full.idxmax(axis=1)

# drop labels from training dataset
data = data.drop(columns="type")



# standardize numerical columns
def standardize(df, col):
    df[col] = (df[col] - df[col].mean()) / df[col].std()


data_st = data.copy()
for i in data_st.iloc[:, :-1].columns:
    standardize(data_st, i)

data_st.head()


x = data_st.values
y = labels_full.values

trainingSetX, testingSetX, trainingSetY, testingSetY = train_test_split(
    x, y, test_size=0.2, random_state=2, shuffle=True, stratify=y
)


print("x training set shape:", trainingSetX.shape)
print("x testing set shape:", testingSetX.shape)
print("y training set row number:", len(trainingSetY))
print("y testing set row number:", len(testingSetY))

from sklearnex import patch_sklearn

patch_sklearn()

superLearner = SuperLearner(
    scorer=accuracy_score,
    random_state=0,
    verbose=10,
    n_jobs=-1,
    backend="multiprocessing",
)

baseLayerLogisticRegression = LogisticRegression(
    max_iter=10000, class_weight="balanced", random_state=2, verbose=3, n_jobs=-1
)


intermediateLayerMLPClassifier = MLPClassifier(
    solver="adam",
    alpha=1e-5,
    hidden_layer_sizes=(5, 5),
    shuffle=True,
    verbose=3,
    random_state=2,
)


intermediateLayerSvm = SVC()

intermediateLayerDecisionTreeClassifier = tree.DecisionTreeClassifier(
    class_weight="balanced",
    max_features="sqrt",
    max_depth=10,
    min_samples_split=0.5,
    random_state=2,
)

metaLayerMLPClassifier = MLPClassifier(
    solver="adam",
    alpha=1e-5,
    hidden_layer_sizes=(12, 12),
    shuffle=True,
    verbose=3,
    random_state=2,
)

superLearner.add(
    [
        baseLayerLogisticRegression,
    ]
)

superLearner.add(
    [intermediateLayerSvm, intermediateLayerDecisionTreeClassifier]
)

superLearner.add_meta(metaLayerMLPClassifier)

superLearner.fit(trainingSetX, trainingSetY)
superLearnerPredicts = superLearner.predict(testingSetX)
superLearnerAcc = accuracy_score(superLearnerPredicts, testingSetY)
